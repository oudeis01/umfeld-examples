# Umfeld vs Processing Visual Comparison Tool

**Visual test runner and report generator for comparing Umfeld and Processing outputs**

This tool builds and runs Umfeld-Processing examples alongside their original Processing counterparts, captures their visual output, and generates comprehensive HTML reports for comparison analysis.

---

## ⚠️ Requirements & Installation

**Linux environments only**

### Dependencies

#### System Requirements
- **Python 3.8+** with pip
- **Processing** with `processing-java` command available
- **X11/Wayland** display server

#### System Packages
Install required system tools:

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install ffmpeg wmctrl x11-utils xclip
```

**Arch Linux:**
```bash
sudo pacman -S ffmpeg wmctrl xorg-xprop xclip
```

#### Python Dependencies
```bash
pip install -r requirements.txt
```

---

## Quick Start

1. **Setup directories:**
   - Place Umfeld examples in `../Processing/`  
   - Ensure `test_props.json` is present
   - Original Processing examples (default: `/opt/processing/modes/java/examples/`)

2. **Run comparison:**
   ```bash
   python main.py
   ```

3. **Generate report:**
   ```bash
   python generate_report.py
   ```

4. **View results:** Open `index.html` in your browser

---

## Tools

### `main.py` - Visual Comparison Runner

Builds, runs, and captures visual output from Umfeld and Processing examples.

**SYNOPSIS**
```
main.py [OPTIONS]
```

**OPTIONS**
- `--project NAME`  
  Run specific project by name (e.g., "conditionals_1")
  
- `--category CATEGORY`  
  Run all projects in category (e.g., "Basics/Image")

- `--failed-list FILE`  
  Run only projects listed in failed projects file (generated by `generate_report.py`)
  
- `--umfeld_example_dir DIR`  
  Umfeld examples directory (default: "../Processing/")
  
- `--processing_example_dir DIR`  
  Processing examples directory (default: "/opt/processing/modes/java/examples/")

**EXAMPLES**
```bash
# Run single project
main.py --project "mouse_signals"

# Run category
main.py --category "Basics/Control"

# Re-run only failed projects
main.py --failed-list failed_projects.txt

# Custom directories
main.py --umfeld_example_dir ./examples --processing_example_dir ./processing_ref
```

**OUTPUT**
- Execution logs: `run_log_YYYYMMDD_HHMMSS.txt`
- Visual outputs: Saved in `comparison/` directory
- Comparison files: `comparison/comparison-*.png` or `comparison/comparison-*.mp4`
- Individual framework files are automatically deleted after concatenation

---

### `generate_report.py` - HTML Report Generator

Generates HTML reports from comparison results with interactive features.

**SYNOPSIS**
```
generate_report.py [OPTIONS]
```

**OPTIONS**
- `--input-dir DIR`  
  Root directory with result files (default: "../Processing/")
  
- `--output FILE`  
  Output HTML filename (default: "index.html")
  
- `--title TITLE`  
  Report title (default: "Umfeld vs Processing Comparison Report")
  
- `--category CATEGORY`  
  Generate report for specific category only
  
- `--props-file FILE`  
  Path to test_props.json (default: "test_props.json")
  
- `--include-logs`  
  Include log file analysis (errors, warnings, failures) in the report. If multiple, then it picks the latest log file

**EXAMPLES**
```bash
# Generate full report
generate_report.py

# Generate report with log analysis
generate_report.py --include-logs

# Category-specific report with logs
generate_report.py --category "Basics/Image" --include-logs

# Custom output
generate_report.py --output my_report.html --title "My Custom Report"
```

**FEATURES**
- Hover previews and modal views of comparison files(image/video)

---

## Project Structure

```
vrtest/
├── main.py                 # Visual comparison runner
├── generate_report.py      # HTML report generator  
├── requirements.txt        # Python dependencies
├── test_props.json        # Project configuration
├── index.html             # Generated report (GitHub Pages ready)
├── failed_projects.txt    # Generated failed projects list
├── run_log_*.txt          # Execution logs
└── comparison/            # Generated comparison files
    ├── comparison-project1.png
    ├── comparison-project2.mp4
    └── ...
```

---


## Notes

- **Linux/X11 only** - macOS/Windows support incomplete  
- **Interactive vs. Animation awareness** via `test_props.json`
- **Mutual exclusivity:** Cannot combine `--project`, `--category`, and `--failed-list` options
- **Failed project re-runs:** Use `generate_report.py` to create `failed_projects.txt`, then `main.py --failed-list failed_projects.txt` to retry only failed projects
- **File management:** Individual framework files are automatically cleaned up after comparison creation
- **Centralized storage:** All comparison results stored in `comparison/` directory for easy management